services:
  zookeeper:
    image: zookeeper:3.8.1
    restart: always
    environment:
      ZOOKEEPER_TICK_TIME: 2000
      ZOO_MY_ID: 1
    networks:
      - kafka

  kafka:
    image: bitnami/kafka:3.4.0
    restart: always
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      ALLOW_PLAINTEXT_LISTENER: "yes"
      KAFKA_ENABLE_KRAFT: "no"
      KAFKA_CFG_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true" 
    networks:
      - kafka

  kafka-ui:
    image: docker.io/provectuslabs/kafka-ui:v0.7.2
    restart: always
    depends_on:
      - kafka
    ports:
      - "5080:8080"
    environment:
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_NAME: dev
    networks:
      - kafka
  topic-creator:
    image: bitnami/kafka:3.4.0
    depends_on:
      - kafka
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "kafka-topics.sh --create --if-not-exists --topic weather_data --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1"
    networks:
      - kafka

  producer:
    build: .
    container_name: producer
    depends_on:
      - kafka
    env_file:
      - .env
    networks:
      - kafka

  #kafka-consumer-spark:
  #  build:
  #    context: ./consumer_to_spark
  #  container_name: kafka-consumer-spark
  #  depends_on:
  #    - kafka
  #  volumes:
  #    - ${DATA_DIR}:/data
  #  networks:
  #    - kafka

  spark-master:
    image: bitnami/spark:3.5.0
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "${SPARK_UI_PORT}:8080"
      - "${SPARK_WORKER_PORT}:7077"
    volumes:
      - "${DATA_DIR}:/data"
    networks:
      - kafka
  spark-job:
    image: bitnami/spark:3.5.0
    container_name: spark-job
    depends_on:
      - kafka
      - spark-master
    command: >
      spark-submit
      --master ${SPARK_MASTER_URL}
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
      /data/spark_kafka_stream.py
    volumes:
      - ${DATA_DIR}:/data
    networks:
      - kafka

  spark-worker-1:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}
    volumes:
      - "${DATA_DIR}:/data"
    networks:
      - kafka
  spark-worker-2:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}
    volumes:
      - "${DATA_DIR}:/data"
    networks:
      - kafka
  jupyter:
    image: jupyter/pyspark-notebook
    container_name: jupyter
    ports:
      - "${JUPYTER_PORT}:8888"
    volumes:
      - "${NOTEBOOK_DIR}:/home/jovyan/work"
      - "${DATA_DIR}:/home/jovyan/data"
    environment:
      - SPARK_MASTER=${SPARK_MASTER_URL}
    networks:
      - kafka
      
networks:
  kafka:
    driver: bridge
