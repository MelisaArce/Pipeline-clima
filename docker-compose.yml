services:
  zookeeper:
    image: zookeeper:3.8.1
    restart: always
    environment:
      ZOOKEEPER_TICK_TIME: 2000
      ZOO_MY_ID: 1
    networks:
      - kafka

  kafka:
    image: bitnami/kafka:3.4.0
    restart: always
    depends_on:
      - zookeeper
    ports:
      - "${KAFKA_PORT}:9092"
    environment:
      ALLOW_PLAINTEXT_LISTENER: "yes"
      KAFKA_ENABLE_KRAFT: "no"
      KAFKA_CFG_ZOOKEEPER_CONNECT: ${KAFKA_ZOOKEEPER}
      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true" 
    networks:
      - kafka

  kafka-ui:
    image: docker.io/provectuslabs/kafka-ui:v0.7.2
    restart: always
    depends_on:
      - kafka
    ports:
      - "${KAFKA_UI_PORT}:8080"
    environment:
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: ${KAFKA_BROKER}
      KAFKA_CLUSTERS_0_NAME: dev
    networks:
      - kafka

  topic-creator:
    image: bitnami/kafka:3.4.0
    depends_on:
      - kafka
    entrypoint: ["/bin/sh", "-c"]
    command: >
      kafka-topics.sh --create --if-not-exists
      --topic ${KAFKA_TOPIC}
      --bootstrap-server ${KAFKA_BROKER}
      --partitions 1
      --replication-factor 1
    networks:
      - kafka

  #kafka-consumer-spark:
  #  build:
  #    context: ./consumer_to_spark
  #  container_name: kafka-consumer-spark
  #  depends_on:
  #    - kafka
  #  volumes:
  #    - ${DATA_DIR}:/data
  #  networks:
  #    - kafka

  spark-master:
    image: bitnami/spark:3.5.0
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "${SPARK_UI_PORT}:8080"
      - "${SPARK_WORKER_PORT}:7077"
    volumes:
      - "${DATA_DIR}:/data"
    networks:
      - kafka
  spark-job:
    image: bitnami/spark:3.5.0
    container_name: spark-job
    depends_on:
      - kafka
      - spark-master
    command: >
      spark-submit
      --master ${SPARK_MASTER_URL}
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.7.3
      /data/spark_kafka_stream.py
    volumes:
      - ${DATA_DIR}:/data
      - ./data/output:/data/output
      - ./data/checkpoint:/data/checkpoint
    environment:
      # - PG_HOST=${PG_HOST}
      # - PG_PORT=${PG_PORT}
      # - PG_DB=${PG_DB}
      # - PG_USER=${PG_USER}
      # - PG_PASSWORD=${PG_PASSWORD}
      # - PG_TABLE_WEATHER=${PG_TABLE_WEATHER}
      - KAFKA_BROKER=${KAFKA_BROKER}
      - KAFKA_TOPIC=${KAFKA_TOPIC}
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}
    networks:
      - kafka
  spark-worker-1:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}
    volumes:
      - "${DATA_DIR}:/data"
    networks:
      - kafka
  spark-worker-2:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}
    volumes:
      - "${DATA_DIR}:/data"
    networks:
      - kafka

# volumes:
#   pg_data:
      
networks:
  kafka:
    external: true
    name: airflow_pipeline_net
