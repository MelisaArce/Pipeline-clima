services:
  zookeeper:
    image: zookeeper:3.8.1
    restart: always
    environment:
      ZOOKEEPER_TICK_TIME: 2000
      ZOO_MY_ID: 1
    networks:
      - kafka

  kafka:
    image: bitnami/kafka:3.4.0
    restart: always
    depends_on:
      - zookeeper
    ports:
      - "${KAFKA_PORT}:9092"
    environment:
      ALLOW_PLAINTEXT_LISTENER: "yes"
      KAFKA_ENABLE_KRAFT: "no"
      KAFKA_CFG_ZOOKEEPER_CONNECT: ${KAFKA_ZOOKEEPER}
      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true" 
    networks:
      - kafka

  kafka-ui:
    image: docker.io/provectuslabs/kafka-ui:v0.7.2
    restart: always
    depends_on:
      - kafka
    ports:
      - "${KAFKA_UI_PORT}:8080"
    environment:
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: ${KAFKA_BROKER}
      KAFKA_CLUSTERS_0_NAME: dev
    networks:
      - kafka

  topic-creator:
    image: bitnami/kafka:3.4.0
    depends_on:
      - kafka
    entrypoint: ["/bin/sh", "-c"]
    command: >
      kafka-topics.sh --create --if-not-exists
      --topic ${KAFKA_TOPIC}
      --bootstrap-server ${KAFKA_BROKER}
      --partitions 1
      --replication-factor 1
    networks:
      - kafka
      
  producer:
    build:
      context: ./producers
    container_name: producer
    depends_on:
      - kafka
    env_file:
      - .env
    networks:
      - kafka

  #kafka-consumer-spark:
  #  build:
  #    context: ./consumer_to_spark
  #  container_name: kafka-consumer-spark
  #  depends_on:
  #    - kafka
  #  volumes:
  #    - ${DATA_DIR}:/data
  #  networks:
  #    - kafka

  spark-master:
    image: bitnami/spark:3.5.0
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "${SPARK_UI_PORT}:8080"
      - "${SPARK_WORKER_PORT}:7077"
    volumes:
      - "${DATA_DIR}:/data"
    networks:
      - kafka
  spark-job:
    image: bitnami/spark:3.5.0
    container_name: spark-job
    depends_on:
      - kafka
      - spark-master
    command: >
      spark-submit
      --master ${SPARK_MASTER_URL}
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.7.3
      /data/spark_kafka_stream.py
    volumes:
      - ${DATA_DIR}:/data
    environment:
      # - PG_HOST=${PG_HOST}
      # - PG_PORT=${PG_PORT}
      # - PG_DB=${PG_DB}
      # - PG_USER=${PG_USER}
      # - PG_PASSWORD=${PG_PASSWORD}
      # - PG_TABLE_WEATHER=${PG_TABLE_WEATHER}
      - KAFKA_BROKER=${KAFKA_BROKER}
      - KAFKA_TOPIC=${KAFKA_TOPIC}
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}
    networks:
      - kafka
  spark-worker-1:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}
    volumes:
      - "${DATA_DIR}:/data"
    networks:
      - kafka
  spark-worker-2:
    image: bitnami/spark:3.5.0
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}
    volumes:
      - "${DATA_DIR}:/data"
    networks:
      - kafka
  # jupyter:
  #   image: jupyter/pyspark-notebook
  #   container_name: jupyter
  #   ports:
  #     - "${JUPYTER_PORT}:8888"
  #   volumes:
  #     - "${NOTEBOOK_DIR}:/home/jovyan/work"
  #     - "${DATA_DIR}:/home/jovyan/data"
  #   environment:
  #     - SPARK_MASTER=${SPARK_MASTER_URL}
  #   networks:
  #     - kafka
  # postgres-weather:
  #   image: postgres:14
  #   container_name: postgres-weather
  #   restart: always
  #   environment:
  #     POSTGRES_USER: ${PG_USER}
  #     POSTGRES_PASSWORD: ${PG_PASSWORD}
  #     POSTGRES_DB: ${PG_DB}
  #   ports:
  #     - "${PG_PORT}:5432"
  #   volumes:
  #     - pg_data:/var/lib/postgresql/data
  #     - ./postgres-init:/docker-entrypoint-initdb.d
  #   networks:
  #     - kafka

# volumes:
#   pg_data:
      
networks:
  kafka:
    external: true
    name: airflow_pipeline_net

#networks:
#  airflow_pipeline_net:
 #   driver: bridge
